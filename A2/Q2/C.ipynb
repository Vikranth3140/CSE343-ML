{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal as signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply a bandpass filter to the audio signal\n",
    "def apply_bandpass_filter(y, sr, low_freq, high_freq):\n",
    "    nyquist = 0.5 * sr\n",
    "    low = low_freq / nyquist\n",
    "    high = high_freq / nyquist\n",
    "    b, a = signal.butter(1, [low, high], btype='band')\n",
    "    y_filtered = signal.lfilter(b, a, y)\n",
    "    return y_filtered\n",
    "\n",
    "# Function to extract relevant audio features\n",
    "def extract_audio_features_with_filters(directory):\n",
    "    features = []\n",
    "    for foldername in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, foldername)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith('.wav'):\n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "                    # Load the audio file\n",
    "                    y, sr = librosa.load(file_path, sr=16000)\n",
    "                    \n",
    "                    # Apply bandpass filter (e.g., 300 Hz to 3400 Hz for human voice)\n",
    "                    y_filtered = apply_bandpass_filter(y, sr, low_freq=300, high_freq=3400)\n",
    "                    \n",
    "                    # Extract MFCC features (using the filtered signal)\n",
    "                    mfcc = librosa.feature.mfcc(y=y_filtered, sr=sr, n_mfcc=13)\n",
    "                    mfcc_mean = np.mean(mfcc, axis=1)  # Mean MFCC across time\n",
    "                    \n",
    "                    # Extract Chroma features\n",
    "                    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "                    chroma_mean = np.mean(chroma, axis=1)\n",
    "                    \n",
    "                    # Extract Spectral features\n",
    "                    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "                    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "                    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "                    spectral_flatness = librosa.feature.spectral_flatness(y=y)\n",
    "                    \n",
    "                    # Calculate the mean and standard deviation for spectral features\n",
    "                    spectral_centroid_mean = np.mean(spectral_centroid)\n",
    "                    spectral_bandwidth_mean = np.mean(spectral_bandwidth)\n",
    "                    spectral_contrast_mean = np.mean(spectral_contrast, axis=1)\n",
    "                    spectral_flatness_mean = np.mean(spectral_flatness)\n",
    "                    \n",
    "                    # Append extracted features to the list\n",
    "                    features.append([\n",
    "                        foldername, \n",
    "                        *mfcc_mean, \n",
    "                        *chroma_mean, \n",
    "                        spectral_centroid_mean, \n",
    "                        spectral_bandwidth_mean, \n",
    "                        *spectral_contrast_mean, \n",
    "                        spectral_flatness_mean\n",
    "                    ])\n",
    "    \n",
    "    # Create a DataFrame for the extracted features\n",
    "    mfcc_columns = [f'MFCC_{i}' for i in range(1, 14)]\n",
    "    chroma_columns = [f'Chroma_{i}' for i in range(1, 13)]\n",
    "    spectral_columns = ['Spectral_Centroid', 'Spectral_Bandwidth', 'Spectral_Contrast1', \n",
    "                        'Spectral_Contrast2', 'Spectral_Contrast3', 'Spectral_Contrast4', \n",
    "                        'Spectral_Contrast5', 'Spectral_Contrast6', 'Spectral_Contrast7', \n",
    "                        'Spectral_Flatness']\n",
    "    \n",
    "    columns = ['Class'] + mfcc_columns + chroma_columns + spectral_columns\n",
    "    return pd.DataFrame(features, columns=columns)\n",
    "\n",
    "# Assuming your dataset is stored in 'dataset_path'\n",
    "dataset_path = 'data'\n",
    "audio_features_df = extract_audio_features_with_filters(dataset_path)\n",
    "\n",
    "# Displaying the first few rows of the extracted features\n",
    "print(audio_features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode class labels as integers (manual label encoding)\n",
    "classes = audio_features_df['Class'].unique()\n",
    "class_to_index = {label: index for index, label in enumerate(classes)}\n",
    "audio_features_df['Class'] = audio_features_df['Class'].map(class_to_index)\n",
    "\n",
    "# Split the dataset into 80% train and 20% test\n",
    "def train_test_split(data, test_size=0.2):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_size)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "# Splitting the dataset\n",
    "train_data, test_data = train_test_split(audio_features_df)\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train_data.drop(columns=['Class']).values\n",
    "y_train = train_data['Class'].values\n",
    "X_test = test_data.drop(columns=['Class']).values\n",
    "y_test = test_data['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        self.mean = np.zeros((len(self.classes), n_features), dtype=np.float64)\n",
    "        self.var = np.zeros((len(self.classes), n_features), dtype=np.float64)\n",
    "        self.priors = np.zeros(len(self.classes), dtype=np.float64)\n",
    "\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.mean[idx, :] = X_c.mean(axis=0)\n",
    "            self.var[idx, :] = X_c.var(axis=0)\n",
    "            self.priors[idx] = X_c.shape[0] / float(n_samples)\n",
    "\n",
    "    def _gaussian(self, class_idx, x):\n",
    "        mean = self.mean[class_idx]\n",
    "        var = self.var[class_idx]\n",
    "        numerator = np.exp(- (x - mean) ** 2 / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            prior = np.log(self.priors[idx])\n",
    "            posterior = np.sum(np.log(self._gaussian(idx, x)))\n",
    "            posterior = prior + posterior\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "# Train Naive Bayes\n",
    "nb = NaiveBayes()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=10):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.n_features = X.shape[1]\n",
    "        self.tree = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        if depth >= self.max_depth or len(np.unique(y)) == 1:\n",
    "            return self._most_common_label(y)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_features, n_features, replace=False)\n",
    "\n",
    "        best_feat, best_thresh = self._best_criteria(X, y, feat_idxs)\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
    "\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return (best_feat, best_thresh, left, right)\n",
    "\n",
    "    def _best_criteria(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_thresh = None, None\n",
    "\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_column)\n",
    "\n",
    "            for thresh in thresholds:\n",
    "                gain = self._information_gain(y, X_column, thresh)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_thresh = thresh\n",
    "\n",
    "        return split_idx, split_thresh\n",
    "\n",
    "    def _information_gain(self, y, X_column, split_thresh):\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        left_idxs, right_idxs = self._split(X_column, split_thresh)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "\n",
    "        n, n_left, n_right = len(y), len(left_idxs), len(right_idxs)\n",
    "        e_left, e_right = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_left / n) * e_left + (n_right / n) * e_right\n",
    "\n",
    "        return parent_entropy - child_entropy\n",
    "\n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(inputs) for inputs in X])\n",
    "\n",
    "    def _predict(self, inputs):\n",
    "        node = self.tree\n",
    "        while isinstance(node, tuple):\n",
    "            if inputs[node[0]] <= node[1]:\n",
    "                node = node[2]\n",
    "            else:\n",
    "                node = node[3]\n",
    "        return node\n",
    "\n",
    "# Train Decision Tree\n",
    "dt = DecisionTree()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth=10):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_trees):\n",
    "            idxs = np.random.choice(len(X), len(X), replace=True)\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X[idxs], y[idxs])\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.array([np.bincount(tree_preds[:, i]).argmax() for i in range(X.shape[0])])\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForest()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        y_ = np.where(y > 0, 1, 0)\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
    "                y_predicted = self._activation_function(linear_output)\n",
    "\n",
    "                update = self.lr * (y_[idx] - y_predicted)\n",
    "                self.weights += update * x_i\n",
    "                self.bias += update\n",
    "\n",
    "    def _activation_function(self, x):\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return self._activation_function(linear_output)\n",
    "\n",
    "# Train Perceptron\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, y_train)\n",
    "y_pred_perceptron = perceptron.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy function\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "# Evaluate models\n",
    "accuracy_nb = accuracy(y_test, y_pred_nb)\n",
    "accuracy_dt = accuracy(y_test, y_pred_dt)\n",
    "accuracy_rf = accuracy(y_test, y_pred_rf)\n",
    "accuracy_perceptron = accuracy(y_test, y_pred_perceptron)\n",
    "\n",
    "# Display results\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_nb * 100:.2f}%\")\n",
    "print(f\"Decision Tree Accuracy: {accuracy_dt * 100:.2f}%\")\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf * 100:.2f}%\")\n",
    "print(f\"Perceptron Accuracy: {accuracy_perceptron * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
