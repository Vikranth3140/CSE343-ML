{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: matplotlib in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: tensorflow in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: pickle-mixin in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (1.0.2)\n",
      "Requirement already satisfied: scikit-learn in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
      "Requirement already satisfied: rich in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/dhruv/Roamify/CSE343-ML/A3/.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install numpy matplotlib tensorflow tensorflow pickle-mixin scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, N, layer_sizes, lr, activation, weight_init, epochs, batch_size):\n",
    "        self.N = N  # Number of layers\n",
    "        self.layer_sizes = layer_sizes  # List of neurons in each layer\n",
    "        self.lr = lr  # Learning rate\n",
    "        self.activation = activation  # Activation function\n",
    "        self.weight_init = weight_init  # Weight initialization method\n",
    "        self.epochs = epochs  # Number of epochs\n",
    "        self.batch_size = batch_size  # Batch size\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights, self.biases = self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        weights = []\n",
    "        biases = []\n",
    "\n",
    "        for i in range(self.N - 1):\n",
    "            if self.weight_init == \"xavier\":\n",
    "                weight = np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * np.sqrt(1 / self.layer_sizes[i])\n",
    "            elif self.weight_init == \"he\":\n",
    "                weight = np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * np.sqrt(2 / self.layer_sizes[i])\n",
    "            else:  # default random initialization\n",
    "                weight = np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * 0.01\n",
    "            bias = np.zeros((1, self.layer_sizes[i + 1]))\n",
    "\n",
    "            weights.append(weight)\n",
    "            biases.append(bias)\n",
    "\n",
    "        return weights, biases\n",
    "\n",
    "    def activation_function(self, z):\n",
    "        if self.activation == \"relu\":\n",
    "            return np.maximum(0, z)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return np.tanh(z)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def activation_derivative(self, z):\n",
    "        if self.activation == \"relu\":\n",
    "            return np.where(z > 0, 1, 0)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return 1 - np.tanh(z) ** 2\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            sig = self.activation_function(z)\n",
    "            return sig * (1 - sig)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "\n",
    "        for i in range(self.N - 2):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            zs.append(z)\n",
    "            activation = self.activation_function(z)\n",
    "            activations.append(activation)\n",
    "\n",
    "        # Output layer with softmax for probabilities\n",
    "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        zs.append(z)\n",
    "        activation = self.softmax(z)\n",
    "        activations.append(activation)\n",
    "\n",
    "        return activations, zs\n",
    "\n",
    "    def backward(self, X, Y, activations, zs):\n",
    "        grads_w = [None] * (self.N - 1)\n",
    "        grads_b = [None] * (self.N - 1)\n",
    "\n",
    "        # Output layer error\n",
    "        delta = activations[-1] - Y  # Assuming Y is one-hot encoded\n",
    "        grads_w[-1] = np.dot(activations[-2].T, delta) / X.shape[0]\n",
    "        grads_b[-1] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]\n",
    "\n",
    "        # Backpropagation through hidden layers\n",
    "        for i in range(self.N - 3, -1, -1):\n",
    "            delta = np.dot(delta, self.weights[i + 1].T) * self.activation_derivative(zs[i])\n",
    "            grads_w[i] = np.dot(activations[i].T, delta) / X.shape[0]\n",
    "            grads_b[i] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]\n",
    "\n",
    "        return grads_w, grads_b\n",
    "\n",
    "    def update_parameters(self, grads_w, grads_b):\n",
    "        for i in range(self.N - 1):\n",
    "            self.weights[i] -= self.lr * grads_w[i]\n",
    "            self.biases[i] -= self.lr * grads_b[i]\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        for epoch in range(self.epochs):\n",
    "            indices = np.arange(X.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            for start in range(0, X.shape[0], self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_indices = indices[start:end]\n",
    "                X_batch, Y_batch = X[batch_indices], Y[batch_indices]\n",
    "\n",
    "                activations, zs = self.forward(X_batch)\n",
    "                grads_w, grads_b = self.backward(X_batch, Y_batch, activations, zs)\n",
    "                self.update_parameters(grads_w, grads_b)\n",
    "\n",
    "            # Calculate and print loss for the current epoch\n",
    "            # Use Y_batch for calculating loss for each batch, or calculate it after each epoch on the entire dataset\n",
    "            # Here, I calculate loss only for the last batch of the epoch for simplicity.\n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                batch_loss = -np.mean(np.sum(Y_batch * np.log(activations[-1] + 1e-8), axis=1))\n",
    "                print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {batch_loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward(X)\n",
    "        return np.argmax(activations[-1], axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        activations, _ = self.forward(X)\n",
    "        return activations[-1]\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        predictions = self.predict(X)\n",
    "        labels = np.argmax(Y, axis=1)  # Assuming Y is one-hot encoded\n",
    "        accuracy = np.mean(predictions == labels)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ActivationFunctions:\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        \"\"\"\n",
    "        Derivative of the sigmoid function.\n",
    "        \"\"\"\n",
    "        sig = ActivationFunctions.sigmoid(z)\n",
    "        return sig * (1 - sig)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        \"\"\"\n",
    "        Tanh activation function.\n",
    "        \"\"\"\n",
    "        return np.tanh(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh_derivative(z):\n",
    "        \"\"\"\n",
    "        Derivative of the tanh function.\n",
    "        \"\"\"\n",
    "        return 1 - np.tanh(z) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        \"\"\"\n",
    "        ReLU activation function.\n",
    "        \"\"\"\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        \"\"\"\n",
    "        Derivative of the ReLU function.\n",
    "        \"\"\"\n",
    "        return np.where(z > 0, 1, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def leaky_relu(z, alpha=0.01):\n",
    "        \"\"\"\n",
    "        Leaky ReLU activation function with a small slope for negative inputs.\n",
    "        \"\"\"\n",
    "        return np.where(z > 0, z, alpha * z)\n",
    "\n",
    "    @staticmethod\n",
    "    def leaky_relu_derivative(z, alpha=0.01):\n",
    "        \"\"\"\n",
    "        Derivative of the Leaky ReLU function.\n",
    "        \"\"\"\n",
    "        return np.where(z > 0, 1, alpha)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        \"\"\"\n",
    "        Softmax activation function for the output layer.\n",
    "        \"\"\"\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Stability improvement\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class WeightInitializer:\n",
    "    \n",
    "    @staticmethod\n",
    "    def zero_init(shape):\n",
    "        \"\"\"\n",
    "        Initializes weights to zero.\n",
    "        \"\"\"\n",
    "        return np.zeros(shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_init(shape, scale=0.01):\n",
    "        \"\"\"\n",
    "        Initializes weights randomly within a uniform distribution.\n",
    "        \"\"\"\n",
    "        return np.random.uniform(-scale, scale, shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def normal_init(shape, scale=1.0):\n",
    "        \"\"\"\n",
    "        Initializes weights using a normal distribution with mean 0 and standard deviation scale.\n",
    "        \"\"\"\n",
    "        return np.random.normal(0, scale, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 22:19:44.967176: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-09 22:19:44.970922: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-09 22:19:44.979486: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731190784.992317 1456917 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731190784.996081 1456917 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-09 22:19:45.011791: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with activation: sigmoid and weight initialization: zero_init\n",
      "Epoch 1/200, Loss: 2.3003\n",
      "Epoch 10/200, Loss: 2.3031\n",
      "Epoch 20/200, Loss: 2.2962\n",
      "Epoch 30/200, Loss: 2.2959\n",
      "Epoch 40/200, Loss: 2.3015\n",
      "Epoch 50/200, Loss: 2.2921\n",
      "Epoch 60/200, Loss: 2.3023\n",
      "Epoch 70/200, Loss: 2.2967\n",
      "Epoch 80/200, Loss: 2.2933\n",
      "Epoch 90/200, Loss: 2.2965\n",
      "Epoch 100/200, Loss: 2.2947\n",
      "Epoch 110/200, Loss: 2.3074\n",
      "Epoch 120/200, Loss: 2.3075\n",
      "Epoch 130/200, Loss: 2.2942\n",
      "Epoch 140/200, Loss: 2.3000\n",
      "Epoch 150/200, Loss: 2.2982\n",
      "Epoch 160/200, Loss: 2.2974\n",
      "Epoch 170/200, Loss: 2.3047\n",
      "Epoch 180/200, Loss: 2.3048\n",
      "Epoch 190/200, Loss: 2.2992\n",
      "Epoch 200/200, Loss: 2.2983\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to NoneType.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m     train_loss_history\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     54\u001b[0m     val_loss_history\u001b[38;5;241m.\u001b[39mappend(val_loss)\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfigurations[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m training_losses[(activation, weight_init)] \u001b[38;5;241m=\u001b[39m train_loss_history\n\u001b[1;32m     58\u001b[0m validation_losses[(activation, weight_init)] \u001b[38;5;241m=\u001b[39m val_loss_history\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to NoneType.__format__"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration dictionary\n",
    "configurations = {\n",
    "    \"num_layers\": 5,\n",
    "    \"layer_sizes\": [784, 256, 128, 64, 10],\n",
    "    \"learning_rate\": 2e-3,\n",
    "    \"epochs\": 200,\n",
    "    \"batch_size\": 128,\n",
    "    \"activation_functions\": [\"sigmoid\", \"tanh\", \"relu\", \"leaky_relu\"],\n",
    "    \"weight_initializations\": [\"zero_init\", \"random_init\", \"normal_init\"]\n",
    "}\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(X, y), (X_test, y_test) = mnist.load_data()\n",
    "X = X.reshape(-1, 784) / 255.0\n",
    "X_test = X_test.reshape(-1, 784) / 255.0\n",
    "Y = np.eye(10)[y]\n",
    "Y_test = np.eye(10)[y_test]\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Store training and validation losses for plotting\n",
    "training_losses = {}\n",
    "validation_losses = {}\n",
    "\n",
    "for activation in configurations[\"activation_functions\"]:\n",
    "    for weight_init in configurations[\"weight_initializations\"]:\n",
    "        \n",
    "        print(f\"Training with activation: {activation} and weight initialization: {weight_init}\")\n",
    "        \n",
    "        model = NeuralNetwork(\n",
    "            N=configurations[\"num_layers\"],\n",
    "            layer_sizes=configurations[\"layer_sizes\"],\n",
    "            lr=configurations[\"learning_rate\"],\n",
    "            activation=activation,\n",
    "            weight_init=weight_init,\n",
    "            epochs=configurations[\"epochs\"],\n",
    "            batch_size=configurations[\"batch_size\"]\n",
    "        )\n",
    "\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "        \n",
    "        for epoch in range(configurations[\"epochs\"]):\n",
    "            train_loss = model.fit(X_train, Y_train)\n",
    "            val_predictions = model.predict_proba(X_val)\n",
    "            val_loss = -np.mean(np.sum(Y_val * np.log(val_predictions + 1e-8), axis=1))\n",
    "            train_loss_history.append(train_loss)\n",
    "            val_loss_history.append(val_loss)\n",
    "            print(f\"Epoch {epoch + 1}/{configurations['epochs']} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        training_losses[(activation, weight_init)] = train_loss_history\n",
    "        validation_losses[(activation, weight_init)] = val_loss_history\n",
    "        \n",
    "        model_filename = f\"model_{activation}_{weight_init}.pkl\"\n",
    "        with open(model_filename, \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        \n",
    "# Plot training and validation loss for each configuration\n",
    "for (activation, weight_init), train_loss_history in training_losses.items():\n",
    "    val_loss_history = validation_losses[(activation, weight_init)]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_loss_history, label=\"Train Loss\")\n",
    "    plt.plot(val_loss_history, label=\"Validation Loss\")\n",
    "    plt.title(f\"Activation: {activation}, Weight Init: {weight_init}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
