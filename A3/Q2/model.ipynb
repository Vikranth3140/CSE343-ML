{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.45.0)\n",
      "Requirement already satisfied: rich in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\vikra\\onedrive\\desktop\\cse343-ml\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy matplotlib tensorflow tensorflow pickle-mixin scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ActivationFunctions:\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        sig = ActivationFunctions.sigmoid(z)\n",
    "        return sig * (1 - sig)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        return np.tanh(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh_derivative(z):\n",
    "        return 1 - np.tanh(z) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        return np.where(z > 0, 1, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def leaky_relu(z, alpha=0.01):\n",
    "        return np.where(z > 0, z, alpha * z)\n",
    "\n",
    "    @staticmethod\n",
    "    def leaky_relu_derivative(z, alpha=0.01):\n",
    "        return np.where(z > 0, 1, alpha)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "class WeightInitializer:\n",
    "    \n",
    "    @staticmethod\n",
    "    def zero_init(shape):\n",
    "        return np.zeros(shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_init(shape, scale=0.01):\n",
    "        return np.random.uniform(-scale, scale, shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def normal_init(shape, scale=1.0):\n",
    "        return np.random.normal(0, scale, shape)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, N, layer_sizes, lr, activation, weight_init, epochs, batch_size):\n",
    "        self.N = N  # Number of layers\n",
    "        self.layer_sizes = layer_sizes  # List of neurons in each layer\n",
    "        self.lr = lr  # Learning rate\n",
    "        self.activation = activation  # Activation function\n",
    "        self.weight_init = weight_init  # Weight initialization method\n",
    "        self.epochs = epochs  # Number of epochs\n",
    "        self.batch_size = batch_size  # Batch size\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights, self.biases = self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        weights = []\n",
    "        biases = []\n",
    "\n",
    "        for i in range(self.N - 1):\n",
    "            if self.weight_init == \"xavier\":\n",
    "                weight = np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * np.sqrt(1 / self.layer_sizes[i])\n",
    "            elif self.weight_init == \"he\":\n",
    "                weight = np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * np.sqrt(2 / self.layer_sizes[i])\n",
    "            elif self.weight_init == \"zero_init\":\n",
    "                weight = WeightInitializer.zero_init((self.layer_sizes[i], self.layer_sizes[i + 1]))\n",
    "            elif self.weight_init == \"random_init\":\n",
    "                weight = WeightInitializer.random_init((self.layer_sizes[i], self.layer_sizes[i + 1]))\n",
    "            elif self.weight_init == \"normal_init\":\n",
    "                weight = WeightInitializer.normal_init((self.layer_sizes[i], self.layer_sizes[i + 1]))\n",
    "            else:\n",
    "                raise ValueError(\"Unknown weight initialization method\")\n",
    "\n",
    "            bias = np.zeros((1, self.layer_sizes[i + 1]))\n",
    "            weights.append(weight)\n",
    "            biases.append(bias)\n",
    "\n",
    "        return weights, biases\n",
    "\n",
    "    def activation_function(self, z):\n",
    "        if self.activation == \"relu\":\n",
    "            return ActivationFunctions.relu(z)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return ActivationFunctions.tanh(z)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return ActivationFunctions.sigmoid(z)\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            return ActivationFunctions.leaky_relu(z)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def activation_derivative(self, z):\n",
    "        if self.activation == \"relu\":\n",
    "            return ActivationFunctions.relu_derivative(z)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return ActivationFunctions.tanh_derivative(z)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return ActivationFunctions.sigmoid_derivative(z)\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            return ActivationFunctions.leaky_relu_derivative(z)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def softmax(self, z):\n",
    "        return ActivationFunctions.softmax(z)\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "\n",
    "        for i in range(self.N - 2):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            zs.append(z)\n",
    "            activation = self.activation_function(z)\n",
    "            activations.append(activation)\n",
    "\n",
    "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        zs.append(z)\n",
    "        activation = self.softmax(z)\n",
    "        activations.append(activation)\n",
    "\n",
    "        return activations, zs\n",
    "\n",
    "    def backward(self, X, Y, activations, zs):\n",
    "        grads_w = [None] * (self.N - 1)\n",
    "        grads_b = [None] * (self.N - 1)\n",
    "\n",
    "        delta = activations[-1] - Y  # Assuming Y is one-hot encoded\n",
    "        grads_w[-1] = np.dot(activations[-2].T, delta) / X.shape[0]\n",
    "        grads_b[-1] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]\n",
    "\n",
    "        for i in range(self.N - 3, -1, -1):\n",
    "            delta = np.dot(delta, self.weights[i + 1].T) * self.activation_derivative(zs[i])\n",
    "            grads_w[i] = np.dot(activations[i].T, delta) / X.shape[0]\n",
    "            grads_b[i] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]\n",
    "\n",
    "        return grads_w, grads_b\n",
    "\n",
    "    def update_parameters(self, grads_w, grads_b):\n",
    "        for i in range(self.N - 1):\n",
    "            self.weights[i] -= self.lr * grads_w[i]\n",
    "            self.biases[i] -= self.lr * grads_b[i]\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        for epoch in range(self.epochs):\n",
    "            indices = np.arange(X.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            for start in range(0, X.shape[0], self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_indices = indices[start:end]\n",
    "                X_batch, Y_batch = X[batch_indices], Y[batch_indices]\n",
    "\n",
    "                activations, zs = self.forward(X_batch)\n",
    "                grads_w, grads_b = self.backward(X_batch, Y_batch, activations, zs)\n",
    "                self.update_parameters(grads_w, grads_b)\n",
    "\n",
    "            full_activations, _ = self.forward(X)\n",
    "            epoch_loss = -np.mean(np.sum(Y * np.log(full_activations[-1] + 1e-8), axis=1))\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "        return epoch_loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward(X)\n",
    "        return np.argmax(activations[-1], axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        activations, _ = self.forward(X)\n",
    "        return activations[-1]\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        predictions = self.predict(X)\n",
    "        labels = np.argmax(Y, axis=1)\n",
    "        accuracy = np.mean(predictions == labels)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# class ActivationFunctions:\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def sigmoid(z):\n",
    "#         \"\"\"\n",
    "#         Sigmoid activation function.\n",
    "#         \"\"\"\n",
    "#         return 1 / (1 + np.exp(-z))\n",
    "\n",
    "#     @staticmethod\n",
    "#     def sigmoid_derivative(z):\n",
    "#         \"\"\"\n",
    "#         Derivative of the sigmoid function.\n",
    "#         \"\"\"\n",
    "#         sig = ActivationFunctions.sigmoid(z)\n",
    "#         return sig * (1 - sig)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def tanh(z):\n",
    "#         \"\"\"\n",
    "#         Tanh activation function.\n",
    "#         \"\"\"\n",
    "#         return np.tanh(z)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def tanh_derivative(z):\n",
    "#         \"\"\"\n",
    "#         Derivative of the tanh function.\n",
    "#         \"\"\"\n",
    "#         return 1 - np.tanh(z) ** 2\n",
    "\n",
    "#     @staticmethod\n",
    "#     def relu(z):\n",
    "#         \"\"\"\n",
    "#         ReLU activation function.\n",
    "#         \"\"\"\n",
    "#         return np.maximum(0, z)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def relu_derivative(z):\n",
    "#         \"\"\"\n",
    "#         Derivative of the ReLU function.\n",
    "#         \"\"\"\n",
    "#         return np.where(z > 0, 1, 0)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def leaky_relu(z, alpha=0.01):\n",
    "#         \"\"\"\n",
    "#         Leaky ReLU activation function with a small slope for negative inputs.\n",
    "#         \"\"\"\n",
    "#         return np.where(z > 0, z, alpha * z)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def leaky_relu_derivative(z, alpha=0.01):\n",
    "#         \"\"\"\n",
    "#         Derivative of the Leaky ReLU function.\n",
    "#         \"\"\"\n",
    "#         return np.where(z > 0, 1, alpha)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def softmax(z):\n",
    "#         \"\"\"\n",
    "#         Softmax activation function for the output layer.\n",
    "#         \"\"\"\n",
    "#         exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Stability improvement\n",
    "#         return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# class WeightInitializer:\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def zero_init(shape):\n",
    "#         \"\"\"\n",
    "#         Initializes weights to zero.\n",
    "#         \"\"\"\n",
    "#         return np.zeros(shape)\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def random_init(shape, scale=0.01):\n",
    "#         \"\"\"\n",
    "#         Initializes weights randomly within a uniform distribution.\n",
    "#         \"\"\"\n",
    "#         return np.random.uniform(-scale, scale, shape)\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def normal_init(shape, scale=1.0):\n",
    "#         \"\"\"\n",
    "#         Initializes weights using a normal distribution with mean 0 and standard deviation scale.\n",
    "#         \"\"\"\n",
    "#         return np.random.normal(0, scale, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
      "Training with activation: sigmoid and weight initialization: zero_init\n",
      "Epoch 1/200, Loss: 2.3012\n",
      "Epoch 2/200, Loss: 2.3011\n",
      "Epoch 3/200, Loss: 2.3011\n",
      "Epoch 4/200, Loss: 2.3011\n",
      "Epoch 5/200, Loss: 2.3011\n",
      "Epoch 6/200, Loss: 2.3011\n",
      "Epoch 7/200, Loss: 2.3011\n",
      "Epoch 8/200, Loss: 2.3011\n",
      "Epoch 9/200, Loss: 2.3011\n",
      "Epoch 10/200, Loss: 2.3011\n",
      "Epoch 11/200, Loss: 2.3011\n",
      "Epoch 12/200, Loss: 2.3011\n",
      "Epoch 13/200, Loss: 2.3011\n",
      "Epoch 14/200, Loss: 2.3011\n",
      "Epoch 15/200, Loss: 2.3011\n",
      "Epoch 16/200, Loss: 2.3011\n",
      "Epoch 17/200, Loss: 2.3011\n",
      "Epoch 18/200, Loss: 2.3011\n",
      "Epoch 19/200, Loss: 2.3011\n",
      "Epoch 20/200, Loss: 2.3011\n",
      "Epoch 21/200, Loss: 2.3011\n",
      "Epoch 22/200, Loss: 2.3011\n",
      "Epoch 23/200, Loss: 2.3011\n",
      "Epoch 24/200, Loss: 2.3011\n",
      "Epoch 25/200, Loss: 2.3011\n",
      "Epoch 26/200, Loss: 2.3011\n",
      "Epoch 27/200, Loss: 2.3011\n",
      "Epoch 28/200, Loss: 2.3011\n",
      "Epoch 29/200, Loss: 2.3011\n",
      "Epoch 30/200, Loss: 2.3011\n",
      "Epoch 31/200, Loss: 2.3011\n",
      "Epoch 32/200, Loss: 2.3011\n",
      "Epoch 33/200, Loss: 2.3011\n",
      "Epoch 34/200, Loss: 2.3011\n",
      "Epoch 35/200, Loss: 2.3011\n",
      "Epoch 36/200, Loss: 2.3011\n",
      "Epoch 37/200, Loss: 2.3011\n",
      "Epoch 38/200, Loss: 2.3011\n",
      "Epoch 39/200, Loss: 2.3011\n",
      "Epoch 40/200, Loss: 2.3011\n",
      "Epoch 41/200, Loss: 2.3011\n",
      "Epoch 42/200, Loss: 2.3011\n",
      "Epoch 43/200, Loss: 2.3011\n",
      "Epoch 44/200, Loss: 2.3011\n",
      "Epoch 45/200, Loss: 2.3011\n",
      "Epoch 46/200, Loss: 2.3011\n",
      "Epoch 47/200, Loss: 2.3011\n",
      "Epoch 48/200, Loss: 2.3011\n",
      "Epoch 49/200, Loss: 2.3011\n",
      "Epoch 50/200, Loss: 2.3011\n",
      "Epoch 51/200, Loss: 2.3011\n",
      "Epoch 52/200, Loss: 2.3011\n",
      "Epoch 53/200, Loss: 2.3011\n",
      "Epoch 54/200, Loss: 2.3011\n",
      "Epoch 55/200, Loss: 2.3011\n",
      "Epoch 56/200, Loss: 2.3011\n",
      "Epoch 57/200, Loss: 2.3011\n",
      "Epoch 58/200, Loss: 2.3011\n",
      "Epoch 59/200, Loss: 2.3011\n",
      "Epoch 60/200, Loss: 2.3011\n",
      "Epoch 61/200, Loss: 2.3011\n",
      "Epoch 62/200, Loss: 2.3011\n",
      "Epoch 63/200, Loss: 2.3011\n",
      "Epoch 64/200, Loss: 2.3011\n",
      "Epoch 65/200, Loss: 2.3011\n",
      "Epoch 66/200, Loss: 2.3011\n",
      "Epoch 67/200, Loss: 2.3011\n",
      "Epoch 68/200, Loss: 2.3011\n",
      "Epoch 69/200, Loss: 2.3011\n",
      "Epoch 70/200, Loss: 2.3011\n",
      "Epoch 71/200, Loss: 2.3011\n",
      "Epoch 72/200, Loss: 2.3011\n",
      "Epoch 73/200, Loss: 2.3011\n",
      "Epoch 74/200, Loss: 2.3011\n",
      "Epoch 75/200, Loss: 2.3011\n",
      "Epoch 76/200, Loss: 2.3011\n",
      "Epoch 77/200, Loss: 2.3011\n",
      "Epoch 78/200, Loss: 2.3011\n",
      "Epoch 79/200, Loss: 2.3011\n",
      "Epoch 80/200, Loss: 2.3011\n",
      "Epoch 81/200, Loss: 2.3011\n",
      "Epoch 82/200, Loss: 2.3011\n",
      "Epoch 83/200, Loss: 2.3011\n",
      "Epoch 84/200, Loss: 2.3011\n",
      "Epoch 85/200, Loss: 2.3011\n",
      "Epoch 86/200, Loss: 2.3011\n",
      "Epoch 87/200, Loss: 2.3011\n",
      "Epoch 88/200, Loss: 2.3011\n",
      "Epoch 89/200, Loss: 2.3011\n",
      "Epoch 90/200, Loss: 2.3011\n",
      "Epoch 91/200, Loss: 2.3011\n",
      "Epoch 92/200, Loss: 2.3011\n",
      "Epoch 93/200, Loss: 2.3011\n",
      "Epoch 94/200, Loss: 2.3011\n",
      "Epoch 95/200, Loss: 2.3011\n",
      "Epoch 96/200, Loss: 2.3011\n",
      "Epoch 97/200, Loss: 2.3011\n",
      "Epoch 98/200, Loss: 2.3011\n",
      "Epoch 99/200, Loss: 2.3011\n",
      "Epoch 100/200, Loss: 2.3011\n",
      "Epoch 101/200, Loss: 2.3011\n",
      "Epoch 102/200, Loss: 2.3011\n",
      "Epoch 103/200, Loss: 2.3011\n",
      "Epoch 104/200, Loss: 2.3011\n",
      "Epoch 105/200, Loss: 2.3011\n",
      "Epoch 106/200, Loss: 2.3011\n",
      "Epoch 107/200, Loss: 2.3011\n",
      "Epoch 108/200, Loss: 2.3011\n",
      "Epoch 109/200, Loss: 2.3011\n",
      "Epoch 110/200, Loss: 2.3011\n",
      "Epoch 111/200, Loss: 2.3011\n",
      "Epoch 112/200, Loss: 2.3011\n",
      "Epoch 113/200, Loss: 2.3011\n",
      "Epoch 114/200, Loss: 2.3011\n",
      "Epoch 115/200, Loss: 2.3011\n",
      "Epoch 116/200, Loss: 2.3011\n",
      "Epoch 117/200, Loss: 2.3011\n",
      "Epoch 118/200, Loss: 2.3011\n",
      "Epoch 119/200, Loss: 2.3011\n",
      "Epoch 120/200, Loss: 2.3011\n",
      "Epoch 121/200, Loss: 2.3011\n",
      "Epoch 122/200, Loss: 2.3011\n",
      "Epoch 123/200, Loss: 2.3011\n",
      "Epoch 124/200, Loss: 2.3011\n",
      "Epoch 125/200, Loss: 2.3011\n",
      "Epoch 126/200, Loss: 2.3011\n",
      "Epoch 127/200, Loss: 2.3011\n",
      "Epoch 128/200, Loss: 2.3011\n",
      "Epoch 129/200, Loss: 2.3011\n",
      "Epoch 130/200, Loss: 2.3011\n",
      "Epoch 131/200, Loss: 2.3011\n",
      "Epoch 132/200, Loss: 2.3011\n",
      "Epoch 133/200, Loss: 2.3011\n",
      "Epoch 134/200, Loss: 2.3011\n",
      "Epoch 135/200, Loss: 2.3011\n",
      "Epoch 136/200, Loss: 2.3011\n",
      "Epoch 137/200, Loss: 2.3011\n",
      "Epoch 138/200, Loss: 2.3011\n",
      "Epoch 139/200, Loss: 2.3011\n",
      "Epoch 140/200, Loss: 2.3011\n",
      "Epoch 141/200, Loss: 2.3011\n",
      "Epoch 142/200, Loss: 2.3011\n",
      "Epoch 143/200, Loss: 2.3011\n",
      "Epoch 144/200, Loss: 2.3011\n",
      "Epoch 145/200, Loss: 2.3011\n",
      "Epoch 146/200, Loss: 2.3011\n",
      "Epoch 147/200, Loss: 2.3011\n",
      "Epoch 148/200, Loss: 2.3011\n",
      "Epoch 149/200, Loss: 2.3011\n",
      "Epoch 150/200, Loss: 2.3011\n",
      "Epoch 151/200, Loss: 2.3011\n",
      "Epoch 152/200, Loss: 2.3011\n",
      "Epoch 153/200, Loss: 2.3011\n",
      "Epoch 154/200, Loss: 2.3011\n",
      "Epoch 155/200, Loss: 2.3011\n",
      "Epoch 156/200, Loss: 2.3011\n",
      "Epoch 157/200, Loss: 2.3011\n",
      "Epoch 158/200, Loss: 2.3011\n",
      "Epoch 159/200, Loss: 2.3011\n",
      "Epoch 160/200, Loss: 2.3011\n",
      "Epoch 161/200, Loss: 2.3011\n",
      "Epoch 162/200, Loss: 2.3011\n",
      "Epoch 163/200, Loss: 2.3011\n",
      "Epoch 164/200, Loss: 2.3011\n",
      "Epoch 165/200, Loss: 2.3011\n",
      "Epoch 166/200, Loss: 2.3011\n",
      "Epoch 167/200, Loss: 2.3011\n",
      "Epoch 168/200, Loss: 2.3011\n",
      "Epoch 169/200, Loss: 2.3011\n",
      "Epoch 170/200, Loss: 2.3011\n",
      "Epoch 171/200, Loss: 2.3011\n",
      "Epoch 172/200, Loss: 2.3011\n",
      "Epoch 173/200, Loss: 2.3011\n",
      "Epoch 174/200, Loss: 2.3011\n",
      "Epoch 175/200, Loss: 2.3011\n",
      "Epoch 176/200, Loss: 2.3011\n",
      "Epoch 177/200, Loss: 2.3011\n",
      "Epoch 178/200, Loss: 2.3011\n",
      "Epoch 179/200, Loss: 2.3011\n",
      "Epoch 180/200, Loss: 2.3011\n",
      "Epoch 181/200, Loss: 2.3011\n",
      "Epoch 182/200, Loss: 2.3011\n",
      "Epoch 183/200, Loss: 2.3011\n",
      "Epoch 184/200, Loss: 2.3011\n",
      "Epoch 185/200, Loss: 2.3011\n",
      "Epoch 186/200, Loss: 2.3011\n",
      "Epoch 187/200, Loss: 2.3011\n",
      "Epoch 188/200, Loss: 2.3011\n",
      "Epoch 189/200, Loss: 2.3011\n",
      "Epoch 190/200, Loss: 2.3011\n",
      "Epoch 191/200, Loss: 2.3011\n",
      "Epoch 192/200, Loss: 2.3011\n",
      "Epoch 193/200, Loss: 2.3011\n",
      "Epoch 194/200, Loss: 2.3011\n",
      "Epoch 195/200, Loss: 2.3011\n",
      "Epoch 196/200, Loss: 2.3011\n",
      "Epoch 197/200, Loss: 2.3011\n",
      "Epoch 198/200, Loss: 2.3011\n",
      "Epoch 199/200, Loss: 2.3011\n",
      "Epoch 200/200, Loss: 2.3011\n",
      "Epoch 1/200 - Train Loss: 2.3011 - Val Loss: 2.3020\n",
      "Epoch 1/200, Loss: 2.3011\n",
      "Epoch 2/200, Loss: 2.3011\n",
      "Epoch 3/200, Loss: 2.3011\n",
      "Epoch 4/200, Loss: 2.3011\n",
      "Epoch 5/200, Loss: 2.3011\n",
      "Epoch 6/200, Loss: 2.3011\n",
      "Epoch 7/200, Loss: 2.3011\n",
      "Epoch 8/200, Loss: 2.3011\n",
      "Epoch 9/200, Loss: 2.3011\n",
      "Epoch 10/200, Loss: 2.3011\n",
      "Epoch 11/200, Loss: 2.3011\n",
      "Epoch 12/200, Loss: 2.3011\n",
      "Epoch 13/200, Loss: 2.3011\n",
      "Epoch 14/200, Loss: 2.3011\n",
      "Epoch 15/200, Loss: 2.3011\n",
      "Epoch 16/200, Loss: 2.3011\n",
      "Epoch 17/200, Loss: 2.3011\n",
      "Epoch 18/200, Loss: 2.3011\n",
      "Epoch 19/200, Loss: 2.3011\n",
      "Epoch 20/200, Loss: 2.3011\n",
      "Epoch 21/200, Loss: 2.3011\n",
      "Epoch 22/200, Loss: 2.3011\n",
      "Epoch 23/200, Loss: 2.3011\n",
      "Epoch 24/200, Loss: 2.3011\n",
      "Epoch 25/200, Loss: 2.3011\n",
      "Epoch 26/200, Loss: 2.3011\n",
      "Epoch 27/200, Loss: 2.3011\n",
      "Epoch 28/200, Loss: 2.3011\n",
      "Epoch 29/200, Loss: 2.3011\n",
      "Epoch 30/200, Loss: 2.3011\n",
      "Epoch 31/200, Loss: 2.3011\n",
      "Epoch 32/200, Loss: 2.3011\n",
      "Epoch 33/200, Loss: 2.3011\n",
      "Epoch 34/200, Loss: 2.3011\n",
      "Epoch 35/200, Loss: 2.3011\n",
      "Epoch 36/200, Loss: 2.3011\n",
      "Epoch 37/200, Loss: 2.3011\n",
      "Epoch 38/200, Loss: 2.3011\n",
      "Epoch 39/200, Loss: 2.3011\n",
      "Epoch 40/200, Loss: 2.3011\n",
      "Epoch 41/200, Loss: 2.3011\n",
      "Epoch 42/200, Loss: 2.3011\n",
      "Epoch 43/200, Loss: 2.3011\n",
      "Epoch 44/200, Loss: 2.3011\n",
      "Epoch 45/200, Loss: 2.3011\n",
      "Epoch 46/200, Loss: 2.3011\n",
      "Epoch 47/200, Loss: 2.3011\n",
      "Epoch 48/200, Loss: 2.3011\n",
      "Epoch 49/200, Loss: 2.3011\n",
      "Epoch 50/200, Loss: 2.3011\n",
      "Epoch 51/200, Loss: 2.3011\n",
      "Epoch 52/200, Loss: 2.3011\n",
      "Epoch 53/200, Loss: 2.3011\n",
      "Epoch 54/200, Loss: 2.3011\n",
      "Epoch 55/200, Loss: 2.3011\n",
      "Epoch 56/200, Loss: 2.3011\n",
      "Epoch 57/200, Loss: 2.3011\n",
      "Epoch 58/200, Loss: 2.3011\n",
      "Epoch 59/200, Loss: 2.3011\n",
      "Epoch 60/200, Loss: 2.3011\n",
      "Epoch 61/200, Loss: 2.3011\n",
      "Epoch 62/200, Loss: 2.3011\n",
      "Epoch 63/200, Loss: 2.3011\n",
      "Epoch 64/200, Loss: 2.3011\n",
      "Epoch 65/200, Loss: 2.3011\n",
      "Epoch 66/200, Loss: 2.3011\n",
      "Epoch 67/200, Loss: 2.3011\n",
      "Epoch 68/200, Loss: 2.3011\n",
      "Epoch 69/200, Loss: 2.3011\n",
      "Epoch 70/200, Loss: 2.3011\n",
      "Epoch 71/200, Loss: 2.3011\n",
      "Epoch 72/200, Loss: 2.3011\n",
      "Epoch 73/200, Loss: 2.3011\n",
      "Epoch 74/200, Loss: 2.3011\n",
      "Epoch 75/200, Loss: 2.3011\n",
      "Epoch 76/200, Loss: 2.3011\n",
      "Epoch 77/200, Loss: 2.3011\n",
      "Epoch 78/200, Loss: 2.3011\n",
      "Epoch 79/200, Loss: 2.3011\n",
      "Epoch 80/200, Loss: 2.3011\n",
      "Epoch 81/200, Loss: 2.3011\n",
      "Epoch 82/200, Loss: 2.3011\n",
      "Epoch 83/200, Loss: 2.3011\n",
      "Epoch 84/200, Loss: 2.3011\n",
      "Epoch 85/200, Loss: 2.3011\n",
      "Epoch 86/200, Loss: 2.3011\n",
      "Epoch 87/200, Loss: 2.3011\n",
      "Epoch 88/200, Loss: 2.3011\n",
      "Epoch 89/200, Loss: 2.3011\n",
      "Epoch 90/200, Loss: 2.3011\n",
      "Epoch 91/200, Loss: 2.3011\n",
      "Epoch 92/200, Loss: 2.3011\n",
      "Epoch 93/200, Loss: 2.3011\n",
      "Epoch 94/200, Loss: 2.3011\n",
      "Epoch 95/200, Loss: 2.3011\n",
      "Epoch 96/200, Loss: 2.3011\n",
      "Epoch 97/200, Loss: 2.3011\n",
      "Epoch 98/200, Loss: 2.3011\n",
      "Epoch 99/200, Loss: 2.3011\n",
      "Epoch 100/200, Loss: 2.3011\n",
      "Epoch 101/200, Loss: 2.3011\n",
      "Epoch 102/200, Loss: 2.3011\n",
      "Epoch 103/200, Loss: 2.3011\n",
      "Epoch 104/200, Loss: 2.3011\n",
      "Epoch 105/200, Loss: 2.3011\n",
      "Epoch 106/200, Loss: 2.3011\n",
      "Epoch 107/200, Loss: 2.3011\n",
      "Epoch 108/200, Loss: 2.3011\n",
      "Epoch 109/200, Loss: 2.3011\n",
      "Epoch 110/200, Loss: 2.3011\n",
      "Epoch 111/200, Loss: 2.3011\n",
      "Epoch 112/200, Loss: 2.3011\n",
      "Epoch 113/200, Loss: 2.3011\n",
      "Epoch 114/200, Loss: 2.3011\n",
      "Epoch 115/200, Loss: 2.3011\n",
      "Epoch 116/200, Loss: 2.3011\n",
      "Epoch 117/200, Loss: 2.3011\n",
      "Epoch 118/200, Loss: 2.3011\n",
      "Epoch 119/200, Loss: 2.3011\n",
      "Epoch 120/200, Loss: 2.3011\n",
      "Epoch 121/200, Loss: 2.3011\n",
      "Epoch 122/200, Loss: 2.3011\n",
      "Epoch 123/200, Loss: 2.3011\n",
      "Epoch 124/200, Loss: 2.3011\n",
      "Epoch 125/200, Loss: 2.3011\n",
      "Epoch 126/200, Loss: 2.3011\n",
      "Epoch 127/200, Loss: 2.3011\n",
      "Epoch 128/200, Loss: 2.3011\n",
      "Epoch 129/200, Loss: 2.3011\n",
      "Epoch 130/200, Loss: 2.3011\n",
      "Epoch 131/200, Loss: 2.3011\n",
      "Epoch 132/200, Loss: 2.3011\n",
      "Epoch 133/200, Loss: 2.3011\n",
      "Epoch 134/200, Loss: 2.3011\n",
      "Epoch 135/200, Loss: 2.3011\n",
      "Epoch 136/200, Loss: 2.3011\n",
      "Epoch 137/200, Loss: 2.3011\n",
      "Epoch 138/200, Loss: 2.3011\n",
      "Epoch 139/200, Loss: 2.3011\n",
      "Epoch 140/200, Loss: 2.3011\n",
      "Epoch 141/200, Loss: 2.3011\n",
      "Epoch 142/200, Loss: 2.3011\n",
      "Epoch 143/200, Loss: 2.3011\n",
      "Epoch 144/200, Loss: 2.3011\n",
      "Epoch 145/200, Loss: 2.3011\n",
      "Epoch 146/200, Loss: 2.3011\n",
      "Epoch 147/200, Loss: 2.3011\n",
      "Epoch 148/200, Loss: 2.3011\n",
      "Epoch 149/200, Loss: 2.3011\n",
      "Epoch 150/200, Loss: 2.3011\n",
      "Epoch 151/200, Loss: 2.3011\n",
      "Epoch 152/200, Loss: 2.3011\n",
      "Epoch 153/200, Loss: 2.3011\n",
      "Epoch 154/200, Loss: 2.3011\n",
      "Epoch 155/200, Loss: 2.3011\n",
      "Epoch 156/200, Loss: 2.3011\n",
      "Epoch 157/200, Loss: 2.3011\n",
      "Epoch 158/200, Loss: 2.3011\n",
      "Epoch 159/200, Loss: 2.3011\n",
      "Epoch 160/200, Loss: 2.3011\n",
      "Epoch 161/200, Loss: 2.3011\n",
      "Epoch 162/200, Loss: 2.3011\n",
      "Epoch 163/200, Loss: 2.3011\n",
      "Epoch 164/200, Loss: 2.3011\n",
      "Epoch 165/200, Loss: 2.3011\n",
      "Epoch 166/200, Loss: 2.3011\n",
      "Epoch 167/200, Loss: 2.3011\n",
      "Epoch 168/200, Loss: 2.3011\n",
      "Epoch 169/200, Loss: 2.3011\n",
      "Epoch 170/200, Loss: 2.3011\n",
      "Epoch 171/200, Loss: 2.3011\n",
      "Epoch 172/200, Loss: 2.3011\n",
      "Epoch 173/200, Loss: 2.3011\n",
      "Epoch 174/200, Loss: 2.3011\n",
      "Epoch 175/200, Loss: 2.3011\n",
      "Epoch 176/200, Loss: 2.3011\n",
      "Epoch 177/200, Loss: 2.3011\n",
      "Epoch 178/200, Loss: 2.3011\n",
      "Epoch 179/200, Loss: 2.3011\n",
      "Epoch 180/200, Loss: 2.3011\n",
      "Epoch 181/200, Loss: 2.3011\n",
      "Epoch 182/200, Loss: 2.3011\n",
      "Epoch 183/200, Loss: 2.3011\n",
      "Epoch 184/200, Loss: 2.3011\n",
      "Epoch 185/200, Loss: 2.3011\n",
      "Epoch 186/200, Loss: 2.3011\n",
      "Epoch 187/200, Loss: 2.3011\n",
      "Epoch 188/200, Loss: 2.3011\n",
      "Epoch 189/200, Loss: 2.3011\n",
      "Epoch 190/200, Loss: 2.3011\n",
      "Epoch 191/200, Loss: 2.3011\n",
      "Epoch 192/200, Loss: 2.3011\n",
      "Epoch 193/200, Loss: 2.3011\n",
      "Epoch 194/200, Loss: 2.3011\n",
      "Epoch 195/200, Loss: 2.3011\n",
      "Epoch 196/200, Loss: 2.3011\n",
      "Epoch 197/200, Loss: 2.3011\n",
      "Epoch 198/200, Loss: 2.3011\n",
      "Epoch 199/200, Loss: 2.3011\n",
      "Epoch 200/200, Loss: 2.3011\n",
      "Epoch 2/200 - Train Loss: 2.3011 - Val Loss: 2.3019\n",
      "Epoch 1/200, Loss: 2.3011\n",
      "Epoch 2/200, Loss: 2.3011\n",
      "Epoch 3/200, Loss: 2.3011\n",
      "Epoch 4/200, Loss: 2.3011\n",
      "Epoch 5/200, Loss: 2.3011\n",
      "Epoch 6/200, Loss: 2.3011\n",
      "Epoch 7/200, Loss: 2.3011\n",
      "Epoch 8/200, Loss: 2.3011\n",
      "Epoch 9/200, Loss: 2.3011\n",
      "Epoch 10/200, Loss: 2.3011\n",
      "Epoch 11/200, Loss: 2.3011\n",
      "Epoch 12/200, Loss: 2.3011\n",
      "Epoch 13/200, Loss: 2.3011\n",
      "Epoch 14/200, Loss: 2.3011\n",
      "Epoch 15/200, Loss: 2.3011\n",
      "Epoch 16/200, Loss: 2.3011\n",
      "Epoch 17/200, Loss: 2.3011\n",
      "Epoch 18/200, Loss: 2.3011\n",
      "Epoch 19/200, Loss: 2.3011\n",
      "Epoch 20/200, Loss: 2.3011\n",
      "Epoch 21/200, Loss: 2.3011\n",
      "Epoch 22/200, Loss: 2.3011\n",
      "Epoch 23/200, Loss: 2.3011\n",
      "Epoch 24/200, Loss: 2.3011\n",
      "Epoch 25/200, Loss: 2.3011\n",
      "Epoch 26/200, Loss: 2.3011\n",
      "Epoch 27/200, Loss: 2.3011\n",
      "Epoch 28/200, Loss: 2.3011\n",
      "Epoch 29/200, Loss: 2.3011\n",
      "Epoch 30/200, Loss: 2.3011\n",
      "Epoch 31/200, Loss: 2.3011\n",
      "Epoch 32/200, Loss: 2.3011\n",
      "Epoch 33/200, Loss: 2.3011\n",
      "Epoch 34/200, Loss: 2.3011\n",
      "Epoch 35/200, Loss: 2.3011\n",
      "Epoch 36/200, Loss: 2.3011\n",
      "Epoch 37/200, Loss: 2.3011\n",
      "Epoch 38/200, Loss: 2.3011\n",
      "Epoch 39/200, Loss: 2.3011\n",
      "Epoch 40/200, Loss: 2.3011\n",
      "Epoch 41/200, Loss: 2.3011\n",
      "Epoch 42/200, Loss: 2.3011\n",
      "Epoch 43/200, Loss: 2.3011\n",
      "Epoch 44/200, Loss: 2.3011\n",
      "Epoch 45/200, Loss: 2.3011\n",
      "Epoch 46/200, Loss: 2.3011\n",
      "Epoch 47/200, Loss: 2.3011\n",
      "Epoch 48/200, Loss: 2.3011\n",
      "Epoch 49/200, Loss: 2.3011\n",
      "Epoch 50/200, Loss: 2.3011\n",
      "Epoch 51/200, Loss: 2.3011\n",
      "Epoch 52/200, Loss: 2.3011\n",
      "Epoch 53/200, Loss: 2.3011\n",
      "Epoch 54/200, Loss: 2.3011\n",
      "Epoch 55/200, Loss: 2.3011\n",
      "Epoch 56/200, Loss: 2.3011\n",
      "Epoch 57/200, Loss: 2.3011\n",
      "Epoch 58/200, Loss: 2.3011\n",
      "Epoch 59/200, Loss: 2.3011\n",
      "Epoch 60/200, Loss: 2.3011\n",
      "Epoch 61/200, Loss: 2.3011\n",
      "Epoch 62/200, Loss: 2.3011\n",
      "Epoch 63/200, Loss: 2.3011\n",
      "Epoch 64/200, Loss: 2.3011\n",
      "Epoch 65/200, Loss: 2.3011\n",
      "Epoch 66/200, Loss: 2.3011\n",
      "Epoch 67/200, Loss: 2.3011\n",
      "Epoch 68/200, Loss: 2.3011\n",
      "Epoch 69/200, Loss: 2.3011\n",
      "Epoch 70/200, Loss: 2.3011\n",
      "Epoch 71/200, Loss: 2.3011\n",
      "Epoch 72/200, Loss: 2.3011\n",
      "Epoch 73/200, Loss: 2.3011\n",
      "Epoch 74/200, Loss: 2.3011\n",
      "Epoch 75/200, Loss: 2.3011\n",
      "Epoch 76/200, Loss: 2.3011\n",
      "Epoch 77/200, Loss: 2.3011\n",
      "Epoch 78/200, Loss: 2.3011\n",
      "Epoch 79/200, Loss: 2.3011\n",
      "Epoch 80/200, Loss: 2.3011\n",
      "Epoch 81/200, Loss: 2.3011\n",
      "Epoch 82/200, Loss: 2.3011\n",
      "Epoch 83/200, Loss: 2.3011\n",
      "Epoch 84/200, Loss: 2.3011\n",
      "Epoch 85/200, Loss: 2.3011\n",
      "Epoch 86/200, Loss: 2.3011\n",
      "Epoch 87/200, Loss: 2.3011\n",
      "Epoch 88/200, Loss: 2.3011\n",
      "Epoch 89/200, Loss: 2.3011\n",
      "Epoch 90/200, Loss: 2.3011\n",
      "Epoch 91/200, Loss: 2.3011\n",
      "Epoch 92/200, Loss: 2.3011\n",
      "Epoch 93/200, Loss: 2.3011\n",
      "Epoch 94/200, Loss: 2.3011\n",
      "Epoch 95/200, Loss: 2.3011\n",
      "Epoch 96/200, Loss: 2.3011\n",
      "Epoch 97/200, Loss: 2.3011\n",
      "Epoch 98/200, Loss: 2.3011\n",
      "Epoch 99/200, Loss: 2.3011\n",
      "Epoch 100/200, Loss: 2.3011\n",
      "Epoch 101/200, Loss: 2.3011\n",
      "Epoch 102/200, Loss: 2.3011\n",
      "Epoch 103/200, Loss: 2.3011\n",
      "Epoch 104/200, Loss: 2.3011\n",
      "Epoch 105/200, Loss: 2.3011\n",
      "Epoch 106/200, Loss: 2.3011\n",
      "Epoch 107/200, Loss: 2.3011\n",
      "Epoch 108/200, Loss: 2.3011\n",
      "Epoch 109/200, Loss: 2.3011\n",
      "Epoch 110/200, Loss: 2.3011\n",
      "Epoch 111/200, Loss: 2.3011\n",
      "Epoch 112/200, Loss: 2.3011\n",
      "Epoch 113/200, Loss: 2.3011\n",
      "Epoch 114/200, Loss: 2.3011\n",
      "Epoch 115/200, Loss: 2.3011\n",
      "Epoch 116/200, Loss: 2.3011\n",
      "Epoch 117/200, Loss: 2.3011\n",
      "Epoch 118/200, Loss: 2.3011\n",
      "Epoch 119/200, Loss: 2.3011\n",
      "Epoch 120/200, Loss: 2.3011\n",
      "Epoch 121/200, Loss: 2.3011\n",
      "Epoch 122/200, Loss: 2.3011\n",
      "Epoch 123/200, Loss: 2.3011\n",
      "Epoch 124/200, Loss: 2.3011\n",
      "Epoch 125/200, Loss: 2.3011\n",
      "Epoch 126/200, Loss: 2.3011\n",
      "Epoch 127/200, Loss: 2.3011\n",
      "Epoch 128/200, Loss: 2.3011\n",
      "Epoch 129/200, Loss: 2.3011\n",
      "Epoch 130/200, Loss: 2.3011\n",
      "Epoch 131/200, Loss: 2.3011\n",
      "Epoch 132/200, Loss: 2.3011\n",
      "Epoch 133/200, Loss: 2.3011\n",
      "Epoch 134/200, Loss: 2.3011\n",
      "Epoch 135/200, Loss: 2.3011\n",
      "Epoch 136/200, Loss: 2.3011\n",
      "Epoch 137/200, Loss: 2.3011\n",
      "Epoch 138/200, Loss: 2.3011\n",
      "Epoch 139/200, Loss: 2.3011\n",
      "Epoch 140/200, Loss: 2.3011\n",
      "Epoch 141/200, Loss: 2.3011\n",
      "Epoch 142/200, Loss: 2.3011\n",
      "Epoch 143/200, Loss: 2.3011\n",
      "Epoch 144/200, Loss: 2.3011\n",
      "Epoch 145/200, Loss: 2.3011\n",
      "Epoch 146/200, Loss: 2.3011\n",
      "Epoch 147/200, Loss: 2.3011\n",
      "Epoch 148/200, Loss: 2.3011\n",
      "Epoch 149/200, Loss: 2.3011\n",
      "Epoch 150/200, Loss: 2.3011\n",
      "Epoch 151/200, Loss: 2.3011\n",
      "Epoch 152/200, Loss: 2.3011\n",
      "Epoch 153/200, Loss: 2.3011\n",
      "Epoch 154/200, Loss: 2.3011\n",
      "Epoch 155/200, Loss: 2.3011\n",
      "Epoch 156/200, Loss: 2.3011\n",
      "Epoch 157/200, Loss: 2.3011\n",
      "Epoch 158/200, Loss: 2.3011\n",
      "Epoch 159/200, Loss: 2.3011\n",
      "Epoch 160/200, Loss: 2.3011\n",
      "Epoch 161/200, Loss: 2.3011\n",
      "Epoch 162/200, Loss: 2.3011\n",
      "Epoch 163/200, Loss: 2.3011\n",
      "Epoch 164/200, Loss: 2.3011\n",
      "Epoch 165/200, Loss: 2.3011\n",
      "Epoch 166/200, Loss: 2.3011\n",
      "Epoch 167/200, Loss: 2.3011\n",
      "Epoch 168/200, Loss: 2.3011\n",
      "Epoch 169/200, Loss: 2.3011\n",
      "Epoch 170/200, Loss: 2.3011\n",
      "Epoch 171/200, Loss: 2.3011\n",
      "Epoch 172/200, Loss: 2.3011\n",
      "Epoch 173/200, Loss: 2.3011\n",
      "Epoch 174/200, Loss: 2.3011\n",
      "Epoch 175/200, Loss: 2.3011\n",
      "Epoch 176/200, Loss: 2.3011\n",
      "Epoch 177/200, Loss: 2.3011\n",
      "Epoch 178/200, Loss: 2.3011\n",
      "Epoch 179/200, Loss: 2.3011\n",
      "Epoch 180/200, Loss: 2.3011\n",
      "Epoch 181/200, Loss: 2.3011\n",
      "Epoch 182/200, Loss: 2.3011\n",
      "Epoch 183/200, Loss: 2.3011\n",
      "Epoch 184/200, Loss: 2.3011\n",
      "Epoch 185/200, Loss: 2.3011\n",
      "Epoch 186/200, Loss: 2.3011\n",
      "Epoch 187/200, Loss: 2.3011\n",
      "Epoch 188/200, Loss: 2.3011\n",
      "Epoch 189/200, Loss: 2.3011\n",
      "Epoch 190/200, Loss: 2.3011\n",
      "Epoch 191/200, Loss: 2.3011\n",
      "Epoch 192/200, Loss: 2.3011\n",
      "Epoch 193/200, Loss: 2.3011\n",
      "Epoch 194/200, Loss: 2.3011\n",
      "Epoch 195/200, Loss: 2.3011\n",
      "Epoch 196/200, Loss: 2.3011\n",
      "Epoch 197/200, Loss: 2.3011\n",
      "Epoch 198/200, Loss: 2.3011\n",
      "Epoch 199/200, Loss: 2.3011\n",
      "Epoch 200/200, Loss: 2.3011\n",
      "Epoch 3/200 - Train Loss: 2.3011 - Val Loss: 2.3019\n",
      "Epoch 1/200, Loss: 2.3011\n",
      "Epoch 2/200, Loss: 2.3011\n",
      "Epoch 3/200, Loss: 2.3011\n",
      "Epoch 4/200, Loss: 2.3011\n",
      "Epoch 5/200, Loss: 2.3011\n",
      "Epoch 6/200, Loss: 2.3011\n",
      "Epoch 7/200, Loss: 2.3011\n",
      "Epoch 8/200, Loss: 2.3011\n",
      "Epoch 9/200, Loss: 2.3011\n",
      "Epoch 10/200, Loss: 2.3011\n",
      "Epoch 11/200, Loss: 2.3011\n",
      "Epoch 12/200, Loss: 2.3011\n",
      "Epoch 13/200, Loss: 2.3011\n",
      "Epoch 14/200, Loss: 2.3011\n",
      "Epoch 15/200, Loss: 2.3011\n",
      "Epoch 16/200, Loss: 2.3011\n",
      "Epoch 17/200, Loss: 2.3011\n",
      "Epoch 18/200, Loss: 2.3011\n",
      "Epoch 19/200, Loss: 2.3011\n",
      "Epoch 20/200, Loss: 2.3011\n",
      "Epoch 21/200, Loss: 2.3011\n",
      "Epoch 22/200, Loss: 2.3011\n",
      "Epoch 23/200, Loss: 2.3011\n",
      "Epoch 24/200, Loss: 2.3011\n",
      "Epoch 25/200, Loss: 2.3011\n",
      "Epoch 26/200, Loss: 2.3011\n",
      "Epoch 27/200, Loss: 2.3011\n",
      "Epoch 28/200, Loss: 2.3011\n",
      "Epoch 29/200, Loss: 2.3011\n",
      "Epoch 30/200, Loss: 2.3011\n",
      "Epoch 31/200, Loss: 2.3011\n",
      "Epoch 32/200, Loss: 2.3011\n",
      "Epoch 33/200, Loss: 2.3011\n",
      "Epoch 34/200, Loss: 2.3011\n",
      "Epoch 35/200, Loss: 2.3011\n",
      "Epoch 36/200, Loss: 2.3011\n",
      "Epoch 37/200, Loss: 2.3011\n",
      "Epoch 38/200, Loss: 2.3011\n",
      "Epoch 39/200, Loss: 2.3011\n",
      "Epoch 40/200, Loss: 2.3011\n",
      "Epoch 41/200, Loss: 2.3011\n",
      "Epoch 42/200, Loss: 2.3011\n",
      "Epoch 43/200, Loss: 2.3011\n",
      "Epoch 44/200, Loss: 2.3011\n",
      "Epoch 45/200, Loss: 2.3011\n",
      "Epoch 46/200, Loss: 2.3011\n",
      "Epoch 47/200, Loss: 2.3011\n",
      "Epoch 48/200, Loss: 2.3011\n",
      "Epoch 49/200, Loss: 2.3011\n",
      "Epoch 50/200, Loss: 2.3011\n",
      "Epoch 51/200, Loss: 2.3011\n",
      "Epoch 52/200, Loss: 2.3011\n",
      "Epoch 53/200, Loss: 2.3011\n",
      "Epoch 54/200, Loss: 2.3011\n",
      "Epoch 55/200, Loss: 2.3011\n",
      "Epoch 56/200, Loss: 2.3011\n",
      "Epoch 57/200, Loss: 2.3011\n",
      "Epoch 58/200, Loss: 2.3011\n",
      "Epoch 59/200, Loss: 2.3011\n",
      "Epoch 60/200, Loss: 2.3011\n",
      "Epoch 61/200, Loss: 2.3011\n",
      "Epoch 62/200, Loss: 2.3011\n",
      "Epoch 63/200, Loss: 2.3011\n",
      "Epoch 64/200, Loss: 2.3011\n",
      "Epoch 65/200, Loss: 2.3011\n",
      "Epoch 66/200, Loss: 2.3011\n",
      "Epoch 67/200, Loss: 2.3011\n",
      "Epoch 68/200, Loss: 2.3011\n",
      "Epoch 69/200, Loss: 2.3011\n",
      "Epoch 70/200, Loss: 2.3011\n",
      "Epoch 71/200, Loss: 2.3011\n",
      "Epoch 72/200, Loss: 2.3011\n",
      "Epoch 73/200, Loss: 2.3011\n",
      "Epoch 74/200, Loss: 2.3011\n",
      "Epoch 75/200, Loss: 2.3011\n",
      "Epoch 76/200, Loss: 2.3011\n",
      "Epoch 77/200, Loss: 2.3011\n",
      "Epoch 78/200, Loss: 2.3011\n",
      "Epoch 79/200, Loss: 2.3011\n",
      "Epoch 80/200, Loss: 2.3011\n",
      "Epoch 81/200, Loss: 2.3011\n",
      "Epoch 82/200, Loss: 2.3011\n",
      "Epoch 83/200, Loss: 2.3011\n",
      "Epoch 84/200, Loss: 2.3011\n",
      "Epoch 85/200, Loss: 2.3011\n",
      "Epoch 86/200, Loss: 2.3011\n",
      "Epoch 87/200, Loss: 2.3011\n",
      "Epoch 88/200, Loss: 2.3011\n",
      "Epoch 89/200, Loss: 2.3011\n",
      "Epoch 90/200, Loss: 2.3011\n",
      "Epoch 91/200, Loss: 2.3011\n",
      "Epoch 92/200, Loss: 2.3011\n",
      "Epoch 93/200, Loss: 2.3011\n",
      "Epoch 94/200, Loss: 2.3011\n",
      "Epoch 95/200, Loss: 2.3011\n",
      "Epoch 96/200, Loss: 2.3011\n",
      "Epoch 97/200, Loss: 2.3011\n",
      "Epoch 98/200, Loss: 2.3011\n",
      "Epoch 99/200, Loss: 2.3011\n",
      "Epoch 100/200, Loss: 2.3011\n",
      "Epoch 101/200, Loss: 2.3011\n",
      "Epoch 102/200, Loss: 2.3011\n",
      "Epoch 103/200, Loss: 2.3011\n",
      "Epoch 104/200, Loss: 2.3011\n",
      "Epoch 105/200, Loss: 2.3011\n",
      "Epoch 106/200, Loss: 2.3011\n",
      "Epoch 107/200, Loss: 2.3011\n",
      "Epoch 108/200, Loss: 2.3011\n",
      "Epoch 109/200, Loss: 2.3011\n",
      "Epoch 110/200, Loss: 2.3011\n",
      "Epoch 111/200, Loss: 2.3011\n",
      "Epoch 112/200, Loss: 2.3011\n",
      "Epoch 113/200, Loss: 2.3011\n",
      "Epoch 114/200, Loss: 2.3011\n",
      "Epoch 115/200, Loss: 2.3011\n",
      "Epoch 116/200, Loss: 2.3011\n",
      "Epoch 117/200, Loss: 2.3011\n",
      "Epoch 118/200, Loss: 2.3011\n",
      "Epoch 119/200, Loss: 2.3011\n",
      "Epoch 120/200, Loss: 2.3011\n",
      "Epoch 121/200, Loss: 2.3011\n",
      "Epoch 122/200, Loss: 2.3011\n",
      "Epoch 123/200, Loss: 2.3011\n",
      "Epoch 124/200, Loss: 2.3011\n",
      "Epoch 125/200, Loss: 2.3011\n",
      "Epoch 126/200, Loss: 2.3011\n",
      "Epoch 127/200, Loss: 2.3011\n",
      "Epoch 128/200, Loss: 2.3011\n",
      "Epoch 129/200, Loss: 2.3011\n",
      "Epoch 130/200, Loss: 2.3011\n",
      "Epoch 131/200, Loss: 2.3011\n",
      "Epoch 132/200, Loss: 2.3011\n",
      "Epoch 133/200, Loss: 2.3011\n",
      "Epoch 134/200, Loss: 2.3011\n",
      "Epoch 135/200, Loss: 2.3011\n",
      "Epoch 136/200, Loss: 2.3011\n",
      "Epoch 137/200, Loss: 2.3011\n",
      "Epoch 138/200, Loss: 2.3011\n",
      "Epoch 139/200, Loss: 2.3011\n",
      "Epoch 140/200, Loss: 2.3011\n",
      "Epoch 141/200, Loss: 2.3011\n",
      "Epoch 142/200, Loss: 2.3011\n",
      "Epoch 143/200, Loss: 2.3011\n",
      "Epoch 144/200, Loss: 2.3011\n",
      "Epoch 145/200, Loss: 2.3011\n",
      "Epoch 146/200, Loss: 2.3011\n",
      "Epoch 147/200, Loss: 2.3011\n",
      "Epoch 148/200, Loss: 2.3011\n",
      "Epoch 149/200, Loss: 2.3011\n",
      "Epoch 150/200, Loss: 2.3011\n",
      "Epoch 151/200, Loss: 2.3011\n",
      "Epoch 152/200, Loss: 2.3011\n",
      "Epoch 153/200, Loss: 2.3011\n",
      "Epoch 154/200, Loss: 2.3011\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration dictionary\n",
    "configurations = {\n",
    "    \"num_layers\": 5,\n",
    "    \"layer_sizes\": [784, 256, 128, 64, 10],\n",
    "    \"learning_rate\": 2e-3,\n",
    "    \"epochs\": 200,\n",
    "    \"batch_size\": 128,\n",
    "    \"activation_functions\": [\"sigmoid\", \"tanh\", \"relu\", \"leaky_relu\"],\n",
    "    \"weight_initializations\": [\"zero_init\", \"random_init\", \"normal_init\"]\n",
    "}\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(X, y), (X_test, y_test) = mnist.load_data()\n",
    "X = X.reshape(-1, 784) / 255.0\n",
    "X_test = X_test.reshape(-1, 784) / 255.0\n",
    "Y = np.eye(10)[y]\n",
    "Y_test = np.eye(10)[y_test]\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Store training and validation losses for plotting\n",
    "training_losses = {}\n",
    "validation_losses = {}\n",
    "\n",
    "for activation in configurations[\"activation_functions\"]:\n",
    "    for weight_init in configurations[\"weight_initializations\"]:\n",
    "        \n",
    "        print(f\"Training with activation: {activation} and weight initialization: {weight_init}\")\n",
    "        \n",
    "        model = NeuralNetwork(\n",
    "            N=configurations[\"num_layers\"],\n",
    "            layer_sizes=configurations[\"layer_sizes\"],\n",
    "            lr=configurations[\"learning_rate\"],\n",
    "            activation=activation,\n",
    "            weight_init=weight_init,\n",
    "            epochs=configurations[\"epochs\"],\n",
    "            batch_size=configurations[\"batch_size\"]\n",
    "        )\n",
    "\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "        \n",
    "        for epoch in range(configurations[\"epochs\"]):\n",
    "            train_loss = model.fit(X_train, Y_train)\n",
    "            val_predictions = model.predict_proba(X_val)\n",
    "            val_loss = -np.mean(np.sum(Y_val * np.log(val_predictions + 1e-8), axis=1))\n",
    "            train_loss_history.append(train_loss)\n",
    "            val_loss_history.append(val_loss)\n",
    "            print(f\"Epoch {epoch + 1}/{configurations['epochs']} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        training_losses[(activation, weight_init)] = train_loss_history\n",
    "        validation_losses[(activation, weight_init)] = val_loss_history\n",
    "        \n",
    "        model_filename = f\"model_{activation}_{weight_init}.pkl\"\n",
    "        with open(model_filename, \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        \n",
    "# Plot training and validation loss for each configuration\n",
    "for (activation, weight_init), train_loss_history in training_losses.items():\n",
    "    val_loss_history = validation_losses[(activation, weight_init)]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_loss_history, label=\"Train Loss\")\n",
    "    plt.plot(val_loss_history, label=\"Validation Loss\")\n",
    "    plt.title(f\"Activation: {activation}, Weight Init: {weight_init}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
