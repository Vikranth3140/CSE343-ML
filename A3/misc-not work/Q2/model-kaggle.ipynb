{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy matplotlib tensorflow tensorflow pickle-mixin scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, N, layer_sizes, lr, activation, weight_init, epochs, batch_size):\n",
    "        self.N = N  # Number of layers\n",
    "        self.layer_sizes = layer_sizes  # List of neurons in each layer\n",
    "        self.lr = lr  # Learning rate\n",
    "        self.activation = activation  # Activation function\n",
    "        self.weight_init = weight_init  # Weight initialization method\n",
    "        self.epochs = epochs  # Number of epochs\n",
    "        self.batch_size = batch_size  # Batch size\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights, self.biases = self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        weights = []\n",
    "        biases = []\n",
    "\n",
    "        for i in range(self.N - 1):\n",
    "            if self.weight_init == \"xavier\":\n",
    "                weight = np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * np.sqrt(1 / self.layer_sizes[i])\n",
    "            elif self.weight_init == \"he\":\n",
    "                weight = np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * np.sqrt(2 / self.layer_sizes[i])\n",
    "            else:  # default random initialization\n",
    "                weight = np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * 0.01\n",
    "            bias = np.zeros((1, self.layer_sizes[i + 1]))\n",
    "\n",
    "            weights.append(weight)\n",
    "            biases.append(bias)\n",
    "\n",
    "        return weights, biases\n",
    "\n",
    "    def activation_function(self, z):\n",
    "        if self.activation == \"relu\":\n",
    "            return np.maximum(0, z)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return np.tanh(z)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def activation_derivative(self, z):\n",
    "        if self.activation == \"relu\":\n",
    "            return np.where(z > 0, 1, 0)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return 1 - np.tanh(z) ** 2\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            sig = self.activation_function(z)\n",
    "            return sig * (1 - sig)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "\n",
    "        for i in range(self.N - 2):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            zs.append(z)\n",
    "            activation = self.activation_function(z)\n",
    "            activations.append(activation)\n",
    "\n",
    "        # Output layer with softmax for probabilities\n",
    "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        zs.append(z)\n",
    "        activation = self.softmax(z)\n",
    "        activations.append(activation)\n",
    "\n",
    "        return activations, zs\n",
    "\n",
    "    def backward(self, X, Y, activations, zs):\n",
    "        grads_w = [None] * (self.N - 1)\n",
    "        grads_b = [None] * (self.N - 1)\n",
    "\n",
    "        # Output layer error\n",
    "        delta = activations[-1] - Y  # Assuming Y is one-hot encoded\n",
    "        grads_w[-1] = np.dot(activations[-2].T, delta) / X.shape[0]\n",
    "        grads_b[-1] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]\n",
    "\n",
    "        # Backpropagation through hidden layers\n",
    "        for i in range(self.N - 3, -1, -1):\n",
    "            delta = np.dot(delta, self.weights[i + 1].T) * self.activation_derivative(zs[i])\n",
    "            grads_w[i] = np.dot(activations[i].T, delta) / X.shape[0]\n",
    "            grads_b[i] = np.sum(delta, axis=0, keepdims=True) / X.shape[0]\n",
    "\n",
    "        return grads_w, grads_b\n",
    "\n",
    "    def update_parameters(self, grads_w, grads_b):\n",
    "        for i in range(self.N - 1):\n",
    "            self.weights[i] -= self.lr * grads_w[i]\n",
    "            self.biases[i] -= self.lr * grads_b[i]\n",
    "\n",
    "    def fit(self, X, Y, X_val=None, Y_val=None, early_stopping=False, patience=10):\n",
    "        epoch_loss = None  # Initialize the loss variable\n",
    "        best_val_loss = float('inf')  # Initialize best validation loss\n",
    "        wait = 0  # Initialize wait counter for early stopping\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            indices = np.arange(X.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            for start in range(0, X.shape[0], self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_indices = indices[start:end]\n",
    "                X_batch, Y_batch = X[batch_indices], Y[batch_indices]\n",
    "\n",
    "                activations, zs = self.forward(X_batch)\n",
    "                grads_w, grads_b = self.backward(X_batch, Y_batch, activations, zs)\n",
    "                self.update_parameters(grads_w, grads_b)\n",
    "\n",
    "            # Calculate training loss for the current epoch\n",
    "            full_activations, _ = self.forward(X)\n",
    "            epoch_loss = -np.mean(np.sum(Y * np.log(full_activations[-1] + 1e-8), axis=1))\n",
    "\n",
    "            # Validation loss calculation\n",
    "            if X_val is not None and Y_val is not None:\n",
    "                val_predictions = self.predict_proba(X_val)\n",
    "                val_loss = -np.mean(np.sum(Y_val * np.log(val_predictions + 1e-8), axis=1))\n",
    "                print(f\"Epoch {epoch + 1}/{self.epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "                # Early stopping check\n",
    "                if early_stopping:\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        wait = 0  # Reset the wait counter\n",
    "                    else:\n",
    "                        wait += 1\n",
    "                        if wait >= patience:\n",
    "                            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                            break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch + 1}/{self.epochs}, Train Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        return epoch_loss  # Return the calculated epoch loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward(X)\n",
    "        return np.argmax(activations[-1], axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        activations, _ = self.forward(X)\n",
    "        return activations[-1]\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        predictions = self.predict(X)\n",
    "        labels = np.argmax(Y, axis=1)  # Assuming Y is one-hot encoded\n",
    "        accuracy = np.mean(predictions == labels)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ActivationFunctions:\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        \"\"\"\n",
    "        Derivative of the sigmoid function.\n",
    "        \"\"\"\n",
    "        sig = ActivationFunctions.sigmoid(z)\n",
    "        return sig * (1 - sig)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        \"\"\"\n",
    "        Tanh activation function.\n",
    "        \"\"\"\n",
    "        return np.tanh(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh_derivative(z):\n",
    "        \"\"\"\n",
    "        Derivative of the tanh function.\n",
    "        \"\"\"\n",
    "        return 1 - np.tanh(z) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        \"\"\"\n",
    "        ReLU activation function.\n",
    "        \"\"\"\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        \"\"\"\n",
    "        Derivative of the ReLU function.\n",
    "        \"\"\"\n",
    "        return np.where(z > 0, 1, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def leaky_relu(z, alpha=0.01):\n",
    "        \"\"\"\n",
    "        Leaky ReLU activation function with a small slope for negative inputs.\n",
    "        \"\"\"\n",
    "        return np.where(z > 0, z, alpha * z)\n",
    "\n",
    "    @staticmethod\n",
    "    def leaky_relu_derivative(z, alpha=0.01):\n",
    "        \"\"\"\n",
    "        Derivative of the Leaky ReLU function.\n",
    "        \"\"\"\n",
    "        return np.where(z > 0, 1, alpha)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        \"\"\"\n",
    "        Softmax activation function for the output layer.\n",
    "        \"\"\"\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Stability improvement\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class WeightInitializer:\n",
    "    \n",
    "    @staticmethod\n",
    "    def zero_init(shape):\n",
    "        \"\"\"\n",
    "        Initializes weights to zero.\n",
    "        \"\"\"\n",
    "        return np.zeros(shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_init(shape, scale=0.01):\n",
    "        \"\"\"\n",
    "        Initializes weights randomly within a uniform distribution.\n",
    "        \"\"\"\n",
    "        return np.random.uniform(-scale, scale, shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def normal_init(shape, scale=1.0):\n",
    "        \"\"\"\n",
    "        Initializes weights using a normal distribution with mean 0 and standard deviation scale.\n",
    "        \"\"\"\n",
    "        return np.random.normal(0, scale, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with activation: sigmoid and weight initialization: zero_init\n",
      "Epoch 1/100, Train Loss: 2.3033, Val Loss: 2.3059\n",
      "Epoch 2/100, Train Loss: 2.3048, Val Loss: 2.3062\n",
      "Epoch 3/100, Train Loss: 2.3059, Val Loss: 2.3077\n",
      "Epoch 4/100, Train Loss: 2.3016, Val Loss: 2.3023\n",
      "Epoch 5/100, Train Loss: 2.3041, Val Loss: 2.3045\n",
      "Epoch 6/100, Train Loss: 2.3035, Val Loss: 2.3047\n",
      "Epoch 7/100, Train Loss: 2.3029, Val Loss: 2.3045\n",
      "Epoch 8/100, Train Loss: 2.3026, Val Loss: 2.3038\n",
      "Epoch 9/100, Train Loss: 2.3026, Val Loss: 2.3038\n",
      "Epoch 10/100, Train Loss: 2.3030, Val Loss: 2.3032\n",
      "Epoch 11/100, Train Loss: 2.3031, Val Loss: 2.3031\n",
      "Epoch 12/100, Train Loss: 2.3029, Val Loss: 2.3018\n",
      "Epoch 13/100, Train Loss: 2.3032, Val Loss: 2.3051\n",
      "Epoch 14/100, Train Loss: 2.3018, Val Loss: 2.3032\n",
      "Epoch 15/100, Train Loss: 2.3016, Val Loss: 2.3024\n",
      "Epoch 16/100, Train Loss: 2.3023, Val Loss: 2.3036\n",
      "Epoch 17/100, Train Loss: 2.3021, Val Loss: 2.3022\n",
      "Epoch 18/100, Train Loss: 2.3015, Val Loss: 2.3022\n",
      "Epoch 19/100, Train Loss: 2.3023, Val Loss: 2.3046\n",
      "Epoch 20/100, Train Loss: 2.3020, Val Loss: 2.3022\n",
      "Epoch 21/100, Train Loss: 2.3019, Val Loss: 2.3033\n",
      "Epoch 22/100, Train Loss: 2.3021, Val Loss: 2.3031\n",
      "Early stopping triggered at epoch 22\n",
      "Epoch 1/100 - Train Loss: 2.3021341678255536 - Val Loss: 2.3031357662826815\n",
      "Epoch 1/100, Train Loss: 2.3022, Val Loss: 2.3025\n",
      "Epoch 2/100, Train Loss: 2.3016, Val Loss: 2.3028\n",
      "Epoch 3/100, Train Loss: 2.3016, Val Loss: 2.3022\n",
      "Epoch 4/100, Train Loss: 2.3020, Val Loss: 2.3024\n",
      "Epoch 5/100, Train Loss: 2.3014, Val Loss: 2.3024\n",
      "Epoch 6/100, Train Loss: 2.3012, Val Loss: 2.3021\n",
      "Epoch 7/100, Train Loss: 2.3013, Val Loss: 2.3024\n",
      "Epoch 8/100, Train Loss: 2.3015, Val Loss: 2.3019\n",
      "Epoch 9/100, Train Loss: 2.3016, Val Loss: 2.3018\n",
      "Epoch 10/100, Train Loss: 2.3013, Val Loss: 2.3026\n",
      "Epoch 11/100, Train Loss: 2.3016, Val Loss: 2.3020\n",
      "Epoch 12/100, Train Loss: 2.3011, Val Loss: 2.3015\n",
      "Epoch 13/100, Train Loss: 2.3011, Val Loss: 2.3017\n",
      "Epoch 14/100, Train Loss: 2.3011, Val Loss: 2.3016\n",
      "Epoch 15/100, Train Loss: 2.3008, Val Loss: 2.3016\n",
      "Epoch 16/100, Train Loss: 2.3007, Val Loss: 2.3019\n",
      "Epoch 17/100, Train Loss: 2.3000, Val Loss: 2.3006\n",
      "Epoch 18/100, Train Loss: 2.2993, Val Loss: 2.3005\n",
      "Epoch 19/100, Train Loss: 2.2982, Val Loss: 2.2990\n",
      "Epoch 20/100, Train Loss: 2.2965, Val Loss: 2.2971\n",
      "Epoch 21/100, Train Loss: 2.2927, Val Loss: 2.2938\n",
      "Epoch 22/100, Train Loss: 2.2861, Val Loss: 2.2871\n",
      "Epoch 23/100, Train Loss: 2.2725, Val Loss: 2.2744\n",
      "Epoch 24/100, Train Loss: 2.2359, Val Loss: 2.2380\n",
      "Epoch 25/100, Train Loss: 2.1046, Val Loss: 2.1088\n",
      "Epoch 26/100, Train Loss: 1.9695, Val Loss: 1.9740\n",
      "Epoch 27/100, Train Loss: 1.8906, Val Loss: 1.8929\n",
      "Epoch 28/100, Train Loss: 1.8148, Val Loss: 1.8139\n",
      "Epoch 29/100, Train Loss: 1.7589, Val Loss: 1.7543\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration dictionary\n",
    "configurations = {\n",
    "    \"num_layers\": 5,\n",
    "    \"layer_sizes\": [784, 256, 128, 64, 10],\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 128,\n",
    "    \"activation_functions\": [\"sigmoid\", \"tanh\", \"relu\", \"leaky_relu\"],\n",
    "    \"weight_initializations\": [\"zero_init\", \"random_init\", \"normal_init\"]\n",
    "}\n",
    "\n",
    "# Functions to load MNIST data from idx files\n",
    "def load_images(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Read the magic number, number of images, rows, and columns\n",
    "        magic, num, rows, cols = np.frombuffer(f.read(16), dtype=np.uint32).byteswap()\n",
    "        # Read image data and reshape\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8).reshape(num, rows * cols)\n",
    "        return images / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "def load_labels(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Read the magic number and number of items\n",
    "        magic, num = np.frombuffer(f.read(8), dtype=np.uint32).byteswap()\n",
    "        # Read label data\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        return labels\n",
    "\n",
    "# Load and preprocess the MNIST dataset from idx files\n",
    "train_images_path = 'dataset/kaggle/train-images.idx3-ubyte'\n",
    "train_labels_path = 'dataset/kaggle/train-labels.idx1-ubyte'\n",
    "test_images_path = 'dataset/kaggle/t10k-images.idx3-ubyte'\n",
    "test_labels_path = 'dataset/kaggle/t10k-labels.idx1-ubyte'\n",
    "\n",
    "X = load_images(train_images_path)\n",
    "y = load_labels(train_labels_path)\n",
    "X_test = load_images(test_images_path)\n",
    "y_test = load_labels(test_labels_path)\n",
    "\n",
    "# One-hot encode the labels\n",
    "Y = np.eye(10)[y]\n",
    "Y_test = np.eye(10)[y_test]\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Store training and validation losses for plotting\n",
    "training_losses = {}\n",
    "validation_losses = {}\n",
    "\n",
    "# Loop over each activation function and weight initialization configuration\n",
    "for activation in configurations[\"activation_functions\"]:\n",
    "    for weight_init in configurations[\"weight_initializations\"]:\n",
    "        \n",
    "        print(f\"Training with activation: {activation} and weight initialization: {weight_init}\")\n",
    "        \n",
    "        model = NeuralNetwork(\n",
    "            N=configurations[\"num_layers\"],\n",
    "            layer_sizes=configurations[\"layer_sizes\"],\n",
    "            lr=configurations[\"learning_rate\"],\n",
    "            activation=activation,\n",
    "            weight_init=weight_init,\n",
    "            epochs=configurations[\"epochs\"],\n",
    "            batch_size=configurations[\"batch_size\"]\n",
    "        )\n",
    "\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "        \n",
    "        for epoch in range(configurations[\"epochs\"]):\n",
    "            # Train the model with early stopping enabled\n",
    "            train_loss = model.fit(X_train, Y_train, X_val=X_val, Y_val=Y_val, early_stopping=True, patience=10)\n",
    "            val_predictions = model.predict_proba(X_val)\n",
    "            val_loss = -np.mean(np.sum(Y_val * np.log(val_predictions + 1e-8), axis=1)) if val_predictions is not None else None\n",
    "            \n",
    "            # Append losses to history lists\n",
    "            train_loss_history.append(train_loss)\n",
    "            val_loss_history.append(val_loss)\n",
    "            \n",
    "            # Print epoch progress with conditional handling of None values\n",
    "            print(f\"Epoch {epoch + 1}/{configurations['epochs']} - Train Loss: {train_loss if train_loss is not None else 'N/A'} - Val Loss: {val_loss if val_loss is not None else 'N/A'}\")\n",
    "        \n",
    "        training_losses[(activation, weight_init)] = train_loss_history\n",
    "        validation_losses[(activation, weight_init)] = val_loss_history\n",
    "        \n",
    "        model_filename = f\"model_{activation}_{weight_init}.pkl\"\n",
    "        with open(model_filename, \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        \n",
    "# Plot training and validation loss for each configuration\n",
    "for (activation, weight_init), train_loss_history in training_losses.items():\n",
    "    val_loss_history = validation_losses[(activation, weight_init)]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_loss_history, label=\"Train Loss\")\n",
    "    plt.plot(val_loss_history, label=\"Validation Loss\")\n",
    "    plt.title(f\"Activation: {activation}, Weight Init: {weight_init}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
