{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv('label.csv')\n",
    "\n",
    "image_directory = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_edges(image):\n",
    "    edges = cv2.Canny(image, 100, 200)\n",
    "    return edges.flatten()\n",
    "\n",
    "# Function to extract ORB features\n",
    "def extract_orb_features(image, max_features=128):\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints, descriptors = orb.detectAndCompute(image, None)\n",
    "    if descriptors is not None:\n",
    "        if descriptors.shape[0] > max_features:\n",
    "            descriptors = descriptors[:max_features, :]\n",
    "        elif descriptors.shape[0] < max_features:\n",
    "            padding = np.zeros((max_features - descriptors.shape[0], descriptors.shape[1]))\n",
    "            descriptors = np.vstack((descriptors, padding))\n",
    "        return descriptors.flatten()\n",
    "    else:\n",
    "        return np.zeros(max_features * 32)\n",
    "\n",
    "# Function to extract HOG features\n",
    "def extract_hog_features(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    features = hog(gray_image, pixels_per_cell=(24, 24), block_norm='L2-Hys')\n",
    "    return features\n",
    "\n",
    "# Function to extract LBP features\n",
    "def extract_lbp_features(image, radii=[1, 2, 3], n_points=8, method='uniform'):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    lbp_features = []\n",
    "    \n",
    "    for radius in radii:\n",
    "        lbp = local_binary_pattern(gray_image, n_points, radius, method=method)\n",
    "        hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "        hist = hist.astype(\"float\")\n",
    "        hist /= (hist.sum() + 1e-7)\n",
    "        lbp_features.extend(hist)\n",
    "    \n",
    "    return np.array(lbp_features) \n",
    "\n",
    "# Function to extract color histogram features\n",
    "# def extract_color_histogram(image, bins=(8, 8, 8)):\n",
    "#     hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "#     hist = cv2.calcHist([hsv_image], [0, 1, 2], None, bins, [0, 180, 0, 256, 0, 256])\n",
    "#     hist = cv2.normalize(hist, hist).flatten()\n",
    "#     return hist\n",
    "\n",
    "# Function to extract color histogram features in RGB space\n",
    "# def extract_color_histogram(image, bins=(8, 8, 8)):\n",
    "#     # Convert BGR to RGB\n",
    "#     rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "#     # Calculate the histogram for the RGB channels\n",
    "#     hist = cv2.calcHist([rgb_image], [0, 1, 2], None, bins, [0, 256, 0, 256, 0, 256])\n",
    "    \n",
    "#     # Normalize the histogram\n",
    "#     hist = cv2.normalize(hist, hist).flatten()\n",
    "    \n",
    "#     return hist\n",
    "\n",
    "def extract_color_histogram(image):\n",
    "    # Load the image\n",
    "    \n",
    "    # Compute the histogram for each color channel (B, G, R)\n",
    "    hist_b = cv2.calcHist([image], [0], None, [256], [0, 256])\n",
    "    hist_g = cv2.calcHist([image], [1], None, [256], [0, 256])\n",
    "    hist_r = cv2.calcHist([image], [2], None, [256], [0, 256])\n",
    "    \n",
    "    # Normalize the histograms\n",
    "    hist_b = cv2.normalize(hist_b, hist_b).flatten()\n",
    "    hist_g = cv2.normalize(hist_g, hist_g).flatten()\n",
    "    hist_r = cv2.normalize(hist_r, hist_r).flatten()\n",
    "\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate([hist_b, hist_g, hist_r])\n",
    "\n",
    "    # print(hist_features.shape)\n",
    "\n",
    "    return hist_features\n",
    "\n",
    "\n",
    "# Combine all features into a single feature vector\n",
    "def extract_combined_features(image, pca_model):\n",
    "    edges = extract_edges(image)\n",
    "    if edges is not None:\n",
    "        edges = pca_model.transform([edges])[0]  # Apply PCA\n",
    "\n",
    "    orb_features = extract_orb_features(image)\n",
    "    hog_features = extract_hog_features(image)\n",
    "    lbp_features = extract_lbp_features(image)\n",
    "    color_histogram = extract_color_histogram(image)\n",
    "\n",
    "    # print(f\"edges shape: {edges.shape}, orb_features shape: {orb_features.shape}, hog_features shape: {hog_features.shape}, lbp_features shape: {lbp_features.shape}, color_histogram shape: {color_histogram.shape}\")\n",
    "\n",
    "    combined_features = np.concatenate((edges, orb_features, hog_features, lbp_features, color_histogram))\n",
    "    return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = \"data/Image_1.jpg\"\n",
    "# image = cv2.imread(image_path)\n",
    "\n",
    "# # Resize the image if needed\n",
    "# resized_image = cv2.resize(image, (250, 200))\n",
    "\n",
    "# # pca = PCA(n_components=100)\n",
    "# # Extract combined features\n",
    "# combined_features = extract_combined_features(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edges = []\n",
    "\n",
    "# First pass: Collect edges to fit PCA\n",
    "for index, row in labels_df.iterrows():\n",
    "    image_path = os.path.join(image_directory, row['filename'])\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Ensure the image is loaded correctly\n",
    "    if image is None:\n",
    "        continue\n",
    "    \n",
    "    # Resize the image if needed\n",
    "    resized_image = cv2.resize(image, (250, 200))\n",
    "    \n",
    "    # Extract edges\n",
    "    edges = extract_edges(resized_image)\n",
    "    print(image_path, edges.shape, end='\\r')\n",
    "\n",
    "    if edges is not None:\n",
    "        all_edges.append(edges)\n",
    "\n",
    "# Fit PCA on the collected edge features\n",
    "all_edges = np.array(all_edges)\n",
    "pca = PCA(n_components=100)  # Choose appropriate number of components\n",
    "pca.fit(all_edges)\n",
    "\n",
    "print(f\"PCA model fitted. {all_edges.shape[0]} samples with {all_edges.shape[1]} features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header_written = False\n",
    "\n",
    "# with open('extracted_features_pca.csv', 'w') as csvfile:\n",
    "#     for index, row in labels_df.iterrows():\n",
    "#         image_path = os.path.join(image_directory, row['filename'])\n",
    "#         image = cv2.imread(image_path)\n",
    "\n",
    "#         # Ensure the image is loaded correctly\n",
    "#         if image is None:\n",
    "#             continue\n",
    "        \n",
    "#         # Resize the image if needed\n",
    "#         resized_image = cv2.resize(image, (250, 200))\n",
    "        \n",
    "#         # Extract combined features\n",
    "#         combined_features = extract_combined_features(resized_image, pca)\n",
    "#         print(image_path, combined_features.shape, end='\\r')\n",
    "\n",
    "#         # Normalize features\n",
    "#         scaler = StandardScaler()\n",
    "#         X = scaler.fit_transform(combined_features)\n",
    "        \n",
    "#         # Convert features to a DataFrame row with label and filename\n",
    "#         combined_row = np.append(X, [row['label'], row['filename']])\n",
    "        \n",
    "#         # Convert to DataFrame\n",
    "#         combined_df = pd.DataFrame([combined_row])\n",
    "        \n",
    "#         # Write the row to the CSV, writing the header only once\n",
    "#         if not header_written:\n",
    "#             combined_df.to_csv(csvfile, header=['feature_' + str(i) for i in range(len(combined_features))] + ['label', 'filename'], index=False, mode='a')\n",
    "#             header_written = True\n",
    "#         else:\n",
    "#             combined_df.to_csv(csvfile, header=False, index=False, mode='a')\n",
    "\n",
    "# print(\"Feature extraction completed and saved to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(row_tuple):\n",
    "    index, row = row_tuple\n",
    "    image_path = os.path.join(image_directory, row['filename'])\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    if image is None:\n",
    "        return None\n",
    "\n",
    "    resized_image = cv2.resize(image, (250, 200))\n",
    "\n",
    "    # Extract combined features\n",
    "    combined_features = extract_combined_features(resized_image, pca)\n",
    "    print(image_path, combined_features.shape, end='\\r')\n",
    "\n",
    "    # Return features along with label and filename\n",
    "    return np.append(combined_features, [row['label'], row['filename']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = []\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    # Directly pass the row tuples from labels_df.iterrows()\n",
    "    results = list(executor.map(process_image, labels_df.iterrows()))\n",
    "\n",
    "# Filter out None results (in case any image failed to load)\n",
    "results = [result for result in results if result is not None]\n",
    "features_list.extend(results)\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "features_df = pd.DataFrame(features_list)\n",
    "\n",
    "# Normalize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features_df.iloc[:, :-2] = scaler.fit_transform(features_df.iloc[:, :-2])\n",
    "\n",
    "# Write to CSV\n",
    "features_df.columns = ['feature_' + str(i) for i in range(features_df.shape[1] - 2)] + ['label', 'filename']\n",
    "features_df.to_csv('extracted_features_pca.csv', index=False)\n",
    "\n",
    "print(\"Feature extraction completed and saved to CSV.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
